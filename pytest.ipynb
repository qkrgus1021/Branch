{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c283c42a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.7.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11020\\4270130663.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;31m#build train LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_lookup\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringLookup\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mStringLookupV2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m   \u001b[0mStringLookupV1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringLookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_vectorization_v1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextVectorization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_vectorization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextVectorization\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mTextVectorizationV2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m   \u001b[0mTextVectorizationV1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextVectorization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\preprocessing\\text_vectorization_v1.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcategory_encoding_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring_lookup_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext_vectorization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mpath_stats\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import pygame\n",
    "mp_holistic = mp.solutions.holistic #holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils #drawing utilites\n",
    "def mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #BGR 2 RGB\n",
    "    image.flags.writeable = False                 #image is no longer writeable\n",
    "    results = model.process(image)                 #make prediction\n",
    "    image.flags.writeable = True                  #image is writeable\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR) #RGB 2 BGR\n",
    "    return image,results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image,results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) #draw pose connections\n",
    "    \n",
    "def extract_keypoints(results):\n",
    "    pose =[]\n",
    "    count=0\n",
    "    for res in results.pose_landmarks.landmark:\n",
    "        if(count==0):\n",
    "            test = np.array([res.x, res.y,res.z])\n",
    "            pose.append(test)\n",
    "        count+=1\n",
    "    pose = np.array(pose).flatten()\n",
    "    return pose    \n",
    "\n",
    "def extract_badkeypoints(results):\n",
    "    pose =[]\n",
    "    pose2 =[]\n",
    "    pose3 =[]\n",
    "    pose4 =[]\n",
    "    pose5 =[]\n",
    "    pose6 =[]\n",
    "    pose7 =[]\n",
    "    pose8 =[]\n",
    "    #대각선 4사분면\n",
    "    pose=(results[0]+0.15,results[1]+0.12,results[2])\n",
    "    #대각선 2사분면\n",
    "    pose2=(results[0]-0.15,results[1]-0.12,results[2])\n",
    "    #대각선 1사분면\n",
    "    pose3=(results[0]+0.15,results[1]-0.12,results[2])\n",
    "    #대각성 3사분면\n",
    "    pose4=(results[0]-0.15,results[1]+0.12,results[2])\n",
    "    #x축\n",
    "    pose5=(results[0]+0.12,results[1],results[2])\n",
    "    pose6=(results[0]-0.12,results[1],results[2])\n",
    "    #y축\n",
    "    pose7=(results[0],results[1]+0.05,results[2])\n",
    "    pose8=(results[0],results[1]-0.05,results[2])\n",
    "    \n",
    "    pose = np.array(pose).flatten()\n",
    "    pose2 = np.array(pose2).flatten()\n",
    "    pose3 = np.array(pose3).flatten()\n",
    "    pose4 = np.array(pose4).flatten()\n",
    "    pose5 = np.array(pose5).flatten()\n",
    "    pose6 = np.array(pose6).flatten()\n",
    "    pose7 = np.array(pose7).flatten()\n",
    "    pose8 = np.array(pose8).flatten()\n",
    "    return pose, pose2, pose3, pose4,pose5,pose6,pose7,pose8\n",
    "\n",
    "colors = [(245,117,16),(117,245,16)]\n",
    "\n",
    "def prob_viz(res,actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    if np.argmax(res)==0:\n",
    "        num=0\n",
    "        prob=res[np.argmax(res)]\n",
    "    else:\n",
    "        num=1\n",
    "        prob=res[np.argmax(res)]   \n",
    "    cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100),90+num*40),colors[num],-1)\n",
    "    cv2.putText(output_frame,actions[num],(0,85+num*40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "    return output_frame\n",
    "\n",
    "#touch event\n",
    "def showBlank(event, x, y, flags, param):\n",
    "    #param is the array i from below\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        cv2.imshow('OpenCV Feed',image)\n",
    "        param[0] = param[0] + 1\n",
    "    elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "        cv2.imshow('OpenCV Feed',image)\n",
    "        param[0] = param[0] + 1\n",
    "\n",
    "# path for exproted data\n",
    "DATA_PATH = os.path.join(\"MP_DATA\")\n",
    "\n",
    "#Action that we try to detect\n",
    "actions = np.array(['good','bad','bad2','bad3','bad4','bad5','bad6','bad7','bad8'])\n",
    "\n",
    "#thirty videos worth of data\n",
    "no_sequences = 5\n",
    "\n",
    "#videos are goint to be 30 frames in length\n",
    "sequence_length =10\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "cap = cv2.VideoCapture(0)\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    #LOOP through sequneces aka videos\n",
    "    for sequence in range(no_sequences):\n",
    "        #Loop through video length aka sequnece length\n",
    "        for frame_num in range(sequence_length):\n",
    "                \n",
    "            #Read feed\n",
    "            ret,frame = cap.read()\n",
    "\n",
    "            #make detection\n",
    "            image , results  = mediapipe_detection(frame,holistic)\n",
    "            draw_landmarks(image,results)\n",
    "            \n",
    "            #프로그램을 시작할 때 메시지와 음성을 출력해준다.\n",
    "            if sequence == 0 and frame_num==0:\n",
    "                i = [0]\n",
    "                cv2.putText(image,\"Touch the Screen.\",(100,100),cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,0,255), 4, cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV Feed',image)\n",
    "                # the mousecallback only needs to be set once\n",
    "                cv2.setMouseCallback('OpenCV Feed', showBlank, i )\n",
    "                # show the initial image for the first time.\n",
    "                while i[0] < 1:    \n",
    "                    cv2.waitKey(10)\n",
    "                    cv2.putText(image,\"Touch the Screen.\",(100,100),cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,0,255), 4, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed',image)\n",
    "                    \n",
    "                pygame.mixer.init()\n",
    "                pygame.mixer.music.load('fixing.mp3')\n",
    "                pygame.mixer.music.play()\n",
    "            j=[0]\n",
    "            #사용자의 모습이 보이지 않았을 경우 r키를 입력하여 다시 실행할 수 있게 한다.\n",
    "            if type(results.pose_landmarks) == type(None):\n",
    "                cv2.setMouseCallback('OpenCV Feed', showBlank, j )\n",
    "                frame_num=frame_num-1\n",
    "                while j[0] < 1:    \n",
    "                    cv2.waitKey(10)\n",
    "                    cv2.putText(image,\"Touch the Screen.\",(100,100),cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,0,255), 4, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed',image)\n",
    "                continue\n",
    "            \n",
    "            #Apply collection logic\n",
    "            if frame_num == 0:\n",
    "                cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 4, cv2.LINE_AA)\n",
    "                    \n",
    "                cv2.putText(image, 'Collectiong frames for {} Video Number {}'.format(action,sequence), (15,12),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 4, cv2.LINE_AA)\n",
    "                #show to screen\n",
    "                cv2.imshow('OpenCV Feed',image)\n",
    "                cv2.waitKey(1000)\n",
    "            else:\n",
    "                cv2.putText(image, 'Collectiong frames for {} Video Number {}'.format(action,sequence), (15,12),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 4,cv2.LINE_AA)\n",
    "                    \n",
    "                #show to screen\n",
    "                cv2.imshow('OpenCV Feed',image)\n",
    "                  \n",
    "            #new export keypoints\n",
    "            keypoints = extract_keypoints(results)\n",
    "            npy_path=os.path.join(DATA_PATH, 'good', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,keypoints)\n",
    "            badkeypoints1,badkeypoints2, badkeypoints3, badkeypoints4, badkeypoints5, badkeypoints6, badkeypoints7, badkeypoints8 = extract_badkeypoints(keypoints)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints1)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad2', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints2)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad3', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints3)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad4', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints4)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad5', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints5)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad6', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints6)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad7', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints7)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad8', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints8)\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#build train LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "sequences , labels = [],[]\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window=[]\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH,action,str(sequence),\"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "X = np.array(sequences)\n",
    "y= to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=1)\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64,return_sequences=True,activation='relu',input_shape=(10,3)))\n",
    "model.add(LSTM(128,return_sequences=True,activation='relu'))\n",
    "model.add(LSTM(64,return_sequences=False,activation='relu'))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "#crossentropy -> 수치로 표시하기에 유리한 방식으로 출력해주기 때문이다.\n",
    "#왜 이러한 구조로 구성하였나? -> \n",
    "#1. 적은 양의 데이터만 사용할 예정이고\n",
    "#2. 빠르게 학습시킬 수 있다는 장점과\n",
    "#3. 실시간으로 평가를 빠르게 내려줄 수 있기 때문입니다.\n",
    "model.fit(X_train,y_train,epochs=500,callbacks=[tb_callback])\n",
    "\n",
    "model.save('action.h5')\n",
    "\n",
    "\n",
    "#1 Net detection variables\n",
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions=[]\n",
    "threshold = 0.4\n",
    "bad_pose_count=0\n",
    "start =0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        #Read feed\n",
    "        ret,frame = cap.read()\n",
    "\n",
    "        #make detection\n",
    "        image , results  = mediapipe_detection(frame,holistic)\n",
    "        \n",
    "        #draw_landmark\n",
    "        draw_landmarks(image,results)\n",
    "        j=[0]\n",
    "        #사용자의 모습이 보이지 않았을 경우 r키를 입력하여 다시 실행할 수 있게 한다.\n",
    "        if type(results.pose_landmarks) == type(None):\n",
    "            cv2.setMouseCallback('OpenCV Feed', showBlank, j )\n",
    "            while j[0] < 1:    \n",
    "                cv2.waitKey(10)\n",
    "                cv2.putText(image,\"Touch the Screen.\",(100,100),cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,0,255), 4, cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV Feed',image)\n",
    "            continue\n",
    "        \n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.insert(0,keypoints)\n",
    "        sequence = sequence[:10]\n",
    "        \n",
    "        if len(sequence) == 10:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "        \n",
    "            #3. vizs logic - 0.4보다 큰 수치를 가졌을 경우에 상태가 바뀌면 바뀐 상태로, 안바뀌면 안바뀐 상태로 \n",
    "            if res[np.argmax(res)] > threshold:\n",
    "                if len(sentence) > 0:\n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                    #나쁜 자세가 5분 정도 지속적으로 유지되었을 때 음성으로 알려준다.\n",
    "                    if actions[np.argmax(res)] !=\"good\" and start == 0:\n",
    "                        start = time.time()\n",
    "                    elif actions[np.argmax(res)] == \"good\":\n",
    "                        start = 0\n",
    "                    elif actions[np.argmax(res)] !=\"good\" :\n",
    "                        dif=time.time()-start\n",
    "                        if dif > 60:\n",
    "                            pygame.mixer.init()\n",
    "                            pygame.mixer.music.load('good.mp3')\n",
    "                            pygame.mixer.music.play()\n",
    "                            start=time.time()\n",
    "\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence)>5:\n",
    "                sentence = sentence[-5:]\n",
    "            \n",
    "            #viz\n",
    "            image = prob_viz(res,actions,image,colors)\n",
    "            #cv2.setMouseCallback(windowName, onMouse, param=None)\n",
    "            #show to screen\n",
    "            cv2.imshow('OpenCV Feed',image)\n",
    "        #breaking\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de375b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
