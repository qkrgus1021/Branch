{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b262883",
   "metadata": {},
   "source": [
    "# import and install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d2fffd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.4.1 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: tensorflow-gpu==2.4.1 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.8.9.1)\n",
      "Requirement already satisfied: sklearn in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (0.15.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.1.2)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.19.5)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.1.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (0.3.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (0.2.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (3.19.4)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.32.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.12)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (0.37.1)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (2.4.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (3.7.4.3)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from mediapipe) (4.5.5.64)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (4.29.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.6.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.1.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (41.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (4.11.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2021.10.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\82102\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c283c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b45a0",
   "metadata": {},
   "source": [
    "# key points using MP holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e758e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic #holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils #drawing utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c065037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #BGR 2 RGB\n",
    "    image.flags.writeable = False                 #image is no longer writeable\n",
    "    results = model.process(image)                 #make prediction\n",
    "    image.flags.writeable = True                  #image is writeable\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR) #RGB 2 BGR\n",
    "    return image,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be254227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image,results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) #draw pose connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f41a8",
   "metadata": {},
   "source": [
    "# 3 extract values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66054751",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20796\\1929051668.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0].visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea5a69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose =[]\n",
    "    count=0\n",
    "    for res in results.pose_landmarks.landmark:\n",
    "        if(res.visibility>0.9 and (count==0 or count==11 or count==12)):\n",
    "            test = np.array([res.x, res.y,res.z])\n",
    "            pose.append(test)\n",
    "        count+=1\n",
    "    pose = np.array(pose).flatten()\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c191ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_badkeypoints(results):\n",
    "    pose =[]\n",
    "    pose2 =[]\n",
    "    pose3 =[]\n",
    "    pose4 =[]\n",
    "    count=0\n",
    "    for res in results:\n",
    "        if(res>0):\n",
    "            v=res-0.2\n",
    "            v2=res+0.2\n",
    "            pose.append(v)\n",
    "            pose2.append(v2)\n",
    "        else:\n",
    "            pose.append(res)\n",
    "            pose2.append(res)\n",
    "    pose3=(results[0]+0.2,results[1]-0.2,results[2],results[3]+0.2,results[4]-0.2,results[5],results[6]+0.2,results[7]-0.2\n",
    "                ,results[8])\n",
    "    pose4=(results[0]-0.2,results[1]+0.2,results[2],results[3]-0.2,results[4]+0.2,results[5],results[6]-0.2,results[7]+0.2\n",
    "                ,results[8])\n",
    "    pose5=(results[0]-0.1,results[1],results[2],results[3]-0.1,results[4],results[5],results[6]-0.1,results[7]\n",
    "                ,results[8])\n",
    "        \n",
    "    pose = np.array(pose).flatten()\n",
    "    pose2 = np.array(pose2).flatten()\n",
    "    pose3 = np.array(pose3).flatten()\n",
    "    pose4 = np.array(pose4).flatten()\n",
    "    pose5 = np.array(pose5).flatten()\n",
    "    return pose, pose2, pose3, pose4, pose5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3332969c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1ef549da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3e057612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d9479e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "23153da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8482ef21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "24cc2f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f813830",
   "metadata": {},
   "source": [
    "# setup folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7307380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for exproted data\n",
    "DATA_PATH = os.path.join(\"MP_DATA\")\n",
    "\n",
    "#Action that we try to detect\n",
    "actions = np.array(['good','bad1','bad2','bad3','bad4','bad5'])\n",
    "\n",
    "#thirty videos worth of data\n",
    "no_sequences = 3\n",
    "\n",
    "#videos are goint to be 30 frames in length\n",
    "sequence_length =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1b3313e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbeb194",
   "metadata": {},
   "source": [
    "# 5 collect keypoint for trainging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "34fc9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    #LOOP through sequneces aka videos\n",
    "    for sequence in range(no_sequences):\n",
    "        #Loop through video length aka sequnece length\n",
    "        for frame_num in range(sequence_length):\n",
    "                \n",
    "            #Read feed\n",
    "            ret,frame = cap.read()\n",
    "\n",
    "            #make detection\n",
    "            image , results  = mediapipe_detection(frame,holistic)\n",
    "            draw_landmarks(image,results)\n",
    "                \n",
    "            #Apply collection logic\n",
    "            if frame_num == 0:\n",
    "                cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 4, cv2.LINE_AA)\n",
    "                    \n",
    "                cv2.putText(image, 'Collectiong frames for {} Video Number {}'.format(action,sequence), (15,12),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 4, cv2.LINE_AA)\n",
    "                #show to screen\n",
    "                cv2.imshow('OpenCV Feed',image)\n",
    "                cv2.waitKey(1000)\n",
    "            else:\n",
    "                cv2.putText(image, 'Collectiong frames for {} Video Number {}'.format(action,sequence), (15,12),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 4,cv2.LINE_AA)\n",
    "                    \n",
    "                #show to screen\n",
    "                cv2.imshow('OpenCV Feed',image)\n",
    "                  \n",
    "                \n",
    "            #new export keypoints\n",
    "            keypoints = extract_keypoints(results)\n",
    "            npy_path=os.path.join(DATA_PATH, 'good', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,keypoints)\n",
    "            badkeypoints1,badkeypoints2, badkeypoints3, badkeypoints4,badkeypoints5 = extract_badkeypoints(keypoints)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad1', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints1)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad2', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints2)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad3', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints3)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad4', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints4)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad5', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints5)\n",
    "            \n",
    "                \n",
    "            #breaking\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2b9ff6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.60382783,  0.53910089, -1.53915215,  0.89506209,  0.86493379,\n",
       "       -0.55764329,  0.31095186,  0.86760604, -0.60566676])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a3236032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.40382783,  0.73910089, -1.53915215,  0.69506209,  1.06493379,\n",
       "       -0.55764329,  0.11095186,  1.06760604, -0.60566676])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badkeypoints4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89538aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62accc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691d11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6254bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5745318f",
   "metadata": {},
   "source": [
    "# 6. preprocess data and create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ef267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e53e9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d2d78ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 0, 'bad1': 1, 'bad2': 2, 'bad3': 3, 'bad4': 4, 'bad5': 5}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "507fc4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences , labels = [],[]\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window=[]\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH,action,str(sequence),\"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f2f8288b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 10, 9)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape\n",
    "#행렬의 길이가 다르면 warning을 발생시킴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "49f94e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df81eb63",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([ 0.61077142,  0.61884946, -1.65579736,  0.85848218,  0.87080282,\n",
       "         -0.76630515,  0.32945901,  0.87804246, -0.81908739]),\n",
       "  array([ 0.60810083,  0.6187104 , -1.51928365,  0.85850841,  0.86740339,\n",
       "         -0.62158251,  0.33106768,  0.87476516, -0.67214793]),\n",
       "  array([ 0.60458076,  0.61153942, -1.45579123,  0.8586728 ,  0.86573029,\n",
       "         -0.58174711,  0.33144146,  0.87270707, -0.64277059]),\n",
       "  array([ 0.6044656 ,  0.61164582, -1.50811946,  0.85871065,  0.86493421,\n",
       "         -0.63410944,  0.33217633,  0.87190628, -0.69114423]),\n",
       "  array([ 0.60400045,  0.61180979, -1.53994823,  0.8586669 ,  0.86474258,\n",
       "         -0.64868516,  0.3324315 ,  0.87105006, -0.70091105]),\n",
       "  array([ 0.60337842,  0.60794854, -1.55424118,  0.85867643,  0.86474556,\n",
       "         -0.65757477,  0.33244297,  0.87023646, -0.71300972]),\n",
       "  array([ 0.60355508,  0.60729259, -1.56336439,  0.85862207,  0.86473012,\n",
       "         -0.65399331,  0.33244392,  0.87014467, -0.71790183]),\n",
       "  array([ 0.60489058,  0.60740966, -1.62462902,  0.85860747,  0.86449158,\n",
       "         -0.66902292,  0.33244509,  0.87016469, -0.74152046]),\n",
       "  array([ 0.60469306,  0.6049481 , -1.58460796,  0.85859174,  0.86444932,\n",
       "         -0.65203655,  0.33240765,  0.86980927, -0.73043549]),\n",
       "  array([ 0.60420835,  0.60336089, -1.52737939,  0.85852385,  0.86392307,\n",
       "         -0.6251415 ,  0.33238977,  0.86941904, -0.70201862])],\n",
       " [array([ 0.6041137 ,  0.60287213, -1.53618169,  0.85850179,  0.8637861 ,\n",
       "         -0.62938964,  0.33233339,  0.86852425, -0.70303684]),\n",
       "  array([ 0.60257691,  0.60176498, -1.56152964,  0.85955209,  0.86367595,\n",
       "         -0.65965664,  0.33193699,  0.86856145, -0.7057327 ]),\n",
       "  array([ 0.60086286,  0.60044819, -1.54965043,  0.86066759,  0.86366373,\n",
       "         -0.65882522,  0.33149251,  0.86849952, -0.70787603]),\n",
       "  array([ 0.59989321,  0.59930259, -1.55490625,  0.86164773,  0.86378092,\n",
       "         -0.66484439,  0.33115292,  0.86770207, -0.70998734]),\n",
       "  array([ 0.5987674 ,  0.59848791, -1.54135823,  0.86232072,  0.86441034,\n",
       "         -0.64614397,  0.33106419,  0.86767739, -0.69538844]),\n",
       "  array([ 0.59849483,  0.59845942, -1.539639  ,  0.86252487,  0.86461908,\n",
       "         -0.63659018,  0.33105209,  0.86754364, -0.6914103 ]),\n",
       "  array([ 0.59831107,  0.59846181, -1.50557387,  0.86248457,  0.86465561,\n",
       "         -0.6067555 ,  0.33104369,  0.86742753, -0.67155653]),\n",
       "  array([ 0.59759891,  0.59838307, -1.4285419 ,  0.86211842,  0.86467713,\n",
       "         -0.55995113,  0.33093923,  0.86735785, -0.62885672]),\n",
       "  array([ 0.59756458,  0.59766847, -1.44910622,  0.8609578 ,  0.86413866,\n",
       "         -0.5672797 ,  0.33092791,  0.86700153, -0.61938941]),\n",
       "  array([ 0.59716165,  0.59779632, -1.441486  ,  0.85939091,  0.86404186,\n",
       "         -0.55672657,  0.33090666,  0.86711419, -0.60808265])],\n",
       " [array([ 0.59706742,  0.59863019, -1.45357072,  0.85874474,  0.86388397,\n",
       "         -0.56777966,  0.3309103 ,  0.86718625, -0.62031233]),\n",
       "  array([ 0.57452327,  0.5988999 , -1.37941194,  0.85823458,  0.86335492,\n",
       "         -0.54827678,  0.33071506,  0.86833608, -0.54513401]),\n",
       "  array([ 0.56443864,  0.59908378, -1.35975766,  0.85795057,  0.86249411,\n",
       "         -0.5407781 ,  0.33063155,  0.86902326, -0.53407937]),\n",
       "  array([ 0.55775464,  0.59909815, -1.27423167,  0.85774326,  0.86229956,\n",
       "         -0.50000674,  0.33064303,  0.86968708, -0.47822294]),\n",
       "  array([ 0.55478519,  0.59927249, -1.24165916,  0.85762566,  0.86169344,\n",
       "         -0.48535079,  0.33063641,  0.8700217 , -0.46442443]),\n",
       "  array([ 0.55435193,  0.59928662, -1.21628833,  0.85754287,  0.86149919,\n",
       "         -0.45689163,  0.33062908,  0.87045616, -0.44729987]),\n",
       "  array([ 0.55437553,  0.59930223, -1.20337045,  0.85753334,  0.86129266,\n",
       "         -0.44554964,  0.33061606,  0.87077206, -0.44230229]),\n",
       "  array([ 0.55446279,  0.59932905, -1.22527528,  0.85754716,  0.86088592,\n",
       "         -0.45417237,  0.33065191,  0.87085986, -0.45116848]),\n",
       "  array([ 0.55451709,  0.59934068, -1.21513534,  0.85747814,  0.86083704,\n",
       "         -0.45008606,  0.33057174,  0.87096292, -0.44865528]),\n",
       "  array([ 0.55471349,  0.59935188, -1.14001048,  0.85744655,  0.86038339,\n",
       "         -0.41185656,  0.33047742,  0.87098163, -0.41079435])]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0b5206f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 10, 9)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b11e9502",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1f306e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "46168f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f3444",
   "metadata": {},
   "source": [
    "# 7 Build and train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8fca20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fce909a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7664cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64,return_sequences=True,activation='relu',input_shape=(10,9)))\n",
    "model.add(LSTM(128,return_sequences=True,activation='relu'))\n",
    "model.add(LSTM(64,return_sequences=False,activation='relu'))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f720eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[0.7,0.2,0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "87044d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df93fe17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0d6acaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#왜 이러한 구조로 구성하였나? -> \n",
    "#1. 적은 양의 데이터만 사용할 예정이고\n",
    "#2. 빠르게 학습시킬 수 있다는 장점과\n",
    "#3. 실시간으로 평가를 빠르게 내려줄 수 있기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cec9423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "#crossentropy -> 수치로 표시하기에 유리한 방식으로 출력해주기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ebeac3",
   "metadata": {},
   "source": [
    " model.fit(X_train,y_train,epochs=100,callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6df1d037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.7906 - categorical_accuracy: 0.4118\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 1s 735ms/step - loss: 1.7882 - categorical_accuracy: 0.1765\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7852 - categorical_accuracy: 0.1765\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7825 - categorical_accuracy: 0.1765\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7793 - categorical_accuracy: 0.1765\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7755 - categorical_accuracy: 0.1765\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.7707 - categorical_accuracy: 0.1765\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7652 - categorical_accuracy: 0.1765\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7586 - categorical_accuracy: 0.1765\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7505 - categorical_accuracy: 0.1765\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.7405 - categorical_accuracy: 0.1765\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7278 - categorical_accuracy: 0.1765\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7113 - categorical_accuracy: 0.1765\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6897 - categorical_accuracy: 0.1765\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6620 - categorical_accuracy: 0.1765\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6245 - categorical_accuracy: 0.1765\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.5741 - categorical_accuracy: 0.3529\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.5068 - categorical_accuracy: 0.3529\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4199 - categorical_accuracy: 0.3529\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.3152 - categorical_accuracy: 0.3529\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2018 - categorical_accuracy: 0.3529\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0968 - categorical_accuracy: 0.5294\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0299 - categorical_accuracy: 0.7059\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.0142 - categorical_accuracy: 0.7059\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8427 - categorical_accuracy: 0.8824\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8497 - categorical_accuracy: 0.7059\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7385 - categorical_accuracy: 0.8824\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6691 - categorical_accuracy: 0.8824\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6212 - categorical_accuracy: 0.8824\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5351 - categorical_accuracy: 0.8824\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4917 - categorical_accuracy: 0.8824\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4157 - categorical_accuracy: 0.8824\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3309 - categorical_accuracy: 0.8824\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2373 - categorical_accuracy: 0.8824\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1360 - categorical_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0781 - categorical_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1307 - categorical_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6570 - categorical_accuracy: 0.5294\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2048 - categorical_accuracy: 0.8235\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1199 - categorical_accuracy: 0.9412\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0514 - categorical_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0811 - categorical_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4449 - categorical_accuracy: 0.7647\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2468 - categorical_accuracy: 0.8824\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.5884 - categorical_accuracy: 0.7059\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2403 - categorical_accuracy: 0.7059\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1255 - categorical_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0913 - categorical_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4271 - categorical_accuracy: 0.7059\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6535 - categorical_accuracy: 0.6471\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6438 - categorical_accuracy: 0.6471\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4142 - categorical_accuracy: 0.8235\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2083 - categorical_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.1368 - categorical_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1388 - categorical_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.1944 - categorical_accuracy: 0.9412\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2622 - categorical_accuracy: 0.8824\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3050 - categorical_accuracy: 0.8824\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2918 - categorical_accuracy: 0.8824\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2416 - categorical_accuracy: 0.8824\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1871 - categorical_accuracy: 0.8824\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1454 - categorical_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1183 - categorical_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1081 - categorical_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1113 - categorical_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1220 - categorical_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1282 - categorical_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1162 - categorical_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0963 - categorical_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0812 - categorical_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0698 - categorical_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0545 - categorical_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0391 - categorical_accuracy: 1.0000\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0313 - categorical_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0297 - categorical_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0279 - categorical_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0231 - categorical_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0182 - categorical_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0148 - categorical_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0132 - categorical_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0121 - categorical_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0104 - categorical_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0081 - categorical_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0060 - categorical_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0045 - categorical_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0036 - categorical_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0032 - categorical_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0031 - categorical_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0031 - categorical_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0031 - categorical_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0028 - categorical_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0025 - categorical_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0021 - categorical_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0018 - categorical_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0017 - categorical_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0016 - categorical_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0015 - categorical_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - categorical_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - categorical_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0010 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x212ce3c69c8>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=100,callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "284293ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmd tensorboard --logdir=. 을 통해서 log와 LSTM 레이어가 어떻게 학습되는지 관찰할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e65798",
   "metadata": {},
   "source": [
    "# 8. make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80ccb29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7, 0.2, 0.1]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "852bfb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9db2bc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad5'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08014564",
   "metadata": {},
   "source": [
    "# 9. save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "975d473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a5d519e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65e2df",
   "metadata": {},
   "source": [
    "# 10. Evaluation using confusion matrix and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7f029e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a5c705a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ff6dd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_train, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d8a31494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[14,  0],\n",
       "        [ 0,  3]],\n",
       "\n",
       "       [[14,  0],\n",
       "        [ 0,  3]],\n",
       "\n",
       "       [[14,  0],\n",
       "        [ 0,  3]],\n",
       "\n",
       "       [[14,  0],\n",
       "        [ 0,  3]],\n",
       "\n",
       "       [[14,  0],\n",
       "        [ 0,  3]],\n",
       "\n",
       "       [[15,  0],\n",
       "        [ 0,  2]]], dtype=int64)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "949f3dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5fb34f",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6b5139a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16),(117,245,16),(16,117,245),(15,116,15),(255,116,15),(255,255,255)]\n",
    "def prob_viz(res,actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100),90+num*40),colors[num],-1)\n",
    "        cv2.putText(output_frame,actions[num],(0,85+num*40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ddec80aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7, 0.2, 0.1]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b4c75b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "#1 Net detection variables\n",
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions=[]\n",
    "threshold = 0.4\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        #Read feed\n",
    "        ret,frame = cap.read()\n",
    "\n",
    "        #make detection\n",
    "        image , results  = mediapipe_detection(frame,holistic)\n",
    "        \n",
    "        #draw_landmark\n",
    "        draw_landmarks(image,results)\n",
    "\n",
    "        #2. prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.insert(0,keypoints)\n",
    "        sequence = sequence[:10]\n",
    "        \n",
    "        if len(sequence) == 10:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "        \n",
    "        #3. vizs logic - 0.4보다 큰 수치를 가졌을 경우에 상태가 바뀌면 바뀐 상태로, 안바뀌면 안바뀐 상태로 \n",
    "        if res[np.argmax(res)] > threshold:\n",
    "            if len(sentence) > 0:\n",
    "                if actions[np.argmax(res)] != sentence[-1]:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "            else:\n",
    "                sentence.append(actions[np.argmax(res)])\n",
    "                    \n",
    "        if len(sentence)>5:\n",
    "            sentence = sentence[-5:]\n",
    "            \n",
    "        #viz\n",
    "        image = prob_viz(res,actions,image,colors)\n",
    "        cv2.rectangle(image, (0,0),(640,40), (245,117,16),-1)\n",
    "        cv2.putText(image, ' '.join(sentence),(3,30), cv2.FONT_HERSHEY_SIMPLEX,1,\n",
    "                   (255,255,255),2,cv2.LINE_AA)\n",
    "        \n",
    "        #show to screen\n",
    "        cv2.imshow('OpenCV Feed',image)\n",
    "        #breaking\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0cffbb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.77106994,  0.55455083, -1.57093263,  0.98291284,\n",
       "          0.89581984, -0.51946706,  0.42568725,  0.88737643,\n",
       "         -0.68295014],\n",
       "        [ 0.77108729,  0.55449498, -1.56668329,  0.98290873,\n",
       "          0.89579952, -0.51899773,  0.42568731,  0.88765031,\n",
       "         -0.67366982],\n",
       "        [ 0.77117032,  0.55454916, -1.56097162,  0.98290884,\n",
       "          0.89563698, -0.51355469,  0.42567593,  0.88780922,\n",
       "         -0.67357385],\n",
       "        [ 0.77136815,  0.55455738, -1.5634712 ,  0.98287535,\n",
       "          0.89549625, -0.50800848,  0.42566076,  0.88842493,\n",
       "         -0.67496264],\n",
       "        [ 0.77194899,  0.55453861, -1.5686568 ,  0.98280782,\n",
       "          0.89544404, -0.51559126,  0.42564774,  0.88887411,\n",
       "         -0.67585647],\n",
       "        [ 0.77332622,  0.55454516, -1.59330094,  0.98279178,\n",
       "          0.89537585, -0.52319878,  0.42588633,  0.88931233,\n",
       "         -0.69508219],\n",
       "        [ 0.77441746,  0.55455977, -1.56486261,  0.9828105 ,\n",
       "          0.89556354, -0.50235397,  0.42590156,  0.88934797,\n",
       "         -0.67632645],\n",
       "        [ 0.77533019,  0.55451554, -1.59214222,  0.98318702,\n",
       "          0.89564174, -0.51494747,  0.42610279,  0.88959575,\n",
       "         -0.69718838],\n",
       "        [ 0.77561986,  0.55452609, -1.59990764,  0.9832648 ,\n",
       "          0.89564377, -0.52233768,  0.42625466,  0.88984734,\n",
       "         -0.69631577]]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9b09f11a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1196282161.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_24676\\1196282161.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    if(model.predict(np.expand_dims(sequence, axis=0)))\u001b[0m\n\u001b[1;37m                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if( model.predict(np.expand_dims(sequence, axis=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "467d56be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a3208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ce784958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9d1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9fcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e571b841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.50510514,  0.57523799, -1.33585072,  0.8504442 ,  0.84953171,\n",
       "        -0.5108431 ,  0.31640014,  0.88596261, -0.45764357]),\n",
       " array([ 0.28994605,  0.59063625, -0.94232875,  0.70112467,  0.81348842,\n",
       "        -0.48816568,  0.21541998,  0.90519696, -0.13850515]),\n",
       " array([ 0.18531999,  0.62626845, -0.88276702,  0.61158442,  0.78733104,\n",
       "        -0.49978966,  0.13356446,  0.91602474, -0.10879368]),\n",
       " array([ 0.12598976,  0.65510923, -0.63006675,  0.56234843,  0.77665555,\n",
       "        -0.47775126,  0.08958142,  0.93116921, -0.03752357]),\n",
       " array([ 0.07712992,  0.66336399, -0.6016711 ,  0.5293985 ,  0.76835257,\n",
       "        -0.55867636,  0.06525694,  0.94813412, -0.07443521]),\n",
       " array([ 0.02917137,  0.68252313, -0.72592342,  0.494445  ,  0.76066649,\n",
       "        -0.74803966,  0.04498962,  0.94270498, -0.07351524]),\n",
       " array([-0.03962031,  0.73452175, -0.81641519,  0.45952097,  0.7645824 ,\n",
       "        -0.9177413 , -0.01852623,  0.94218159, -0.11136822]),\n",
       " array([-0.05298466,  0.76346779, -0.81368703,  0.43507344,  0.76759779,\n",
       "        -0.92321539, -0.02840781,  0.93777925, -0.11350026]),\n",
       " array([ 0.41277805,  0.76407897, -0.84968215, -0.05684854,  0.93634742,\n",
       "        -0.13768446])]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "29a43ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "43be3895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "297745bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.expand_dims(keypoints,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b8b59cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c43c81c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55471349,  0.59935188, -1.14001048,  0.85744655,  0.86038339,\n",
       "        -0.41185656,  0.33047742,  0.87098163, -0.41079435]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f2f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
