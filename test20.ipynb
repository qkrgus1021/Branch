{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c5cb332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 7s 767ms/step - loss: 2.1980 - categorical_accuracy: 0.0407\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.1974 - categorical_accuracy: 0.1174\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.1972 - categorical_accuracy: 0.1070\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 2.1956 - categorical_accuracy: 0.1941\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.1956 - categorical_accuracy: 0.1174\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 2.1941 - categorical_accuracy: 0.1278\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.1955 - categorical_accuracy: 0.1222\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 2.1949 - categorical_accuracy: 0.2244\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.1930 - categorical_accuracy: 0.2045\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.1929 - categorical_accuracy: 0.2140\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 2.1909 - categorical_accuracy: 0.2244\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.1895 - categorical_accuracy: 0.2453\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 2.1868 - categorical_accuracy: 0.2093\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.1845 - categorical_accuracy: 0.2557\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 2.1811 - categorical_accuracy: 0.2093\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.1776 - categorical_accuracy: 0.1326\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 2.1699 - categorical_accuracy: 0.1477\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.1580 - categorical_accuracy: 0.2140\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 2.1401 - categorical_accuracy: 0.2348\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.1180 - categorical_accuracy: 0.2244\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.0702 - categorical_accuracy: 0.2140\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 1.9876 - categorical_accuracy: 0.2244\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 1.9155 - categorical_accuracy: 0.2348\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 1.9532 - categorical_accuracy: 0.2140\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.8444 - categorical_accuracy: 0.2652\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.7526 - categorical_accuracy: 0.2907\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.6548 - categorical_accuracy: 0.3267\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 1.6279 - categorical_accuracy: 0.3210\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.5283 - categorical_accuracy: 0.3116\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 1.7325 - categorical_accuracy: 0.2443\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.5801 - categorical_accuracy: 0.2500\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.4494 - categorical_accuracy: 0.4489\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.5240 - categorical_accuracy: 0.3210\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.3856 - categorical_accuracy: 0.3778\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.3722 - categorical_accuracy: 0.3722\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 1.3099 - categorical_accuracy: 0.3011\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.1931 - categorical_accuracy: 0.4081\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.3543 - categorical_accuracy: 0.2652\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.1416 - categorical_accuracy: 0.3731\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.0427 - categorical_accuracy: 0.6278\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.1843 - categorical_accuracy: 0.3210\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 1.1171 - categorical_accuracy: 0.3722\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.9680 - categorical_accuracy: 0.6023\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.9364 - categorical_accuracy: 0.5511\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.8854 - categorical_accuracy: 0.7093\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.8662 - categorical_accuracy: 0.6884\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.7994 - categorical_accuracy: 0.6941\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.8306 - categorical_accuracy: 0.6231\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.6941 - categorical_accuracy: 0.7093\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 1.2819 - categorical_accuracy: 0.3523\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.8126 - categorical_accuracy: 0.5966\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7895 - categorical_accuracy: 0.6222\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.9217 - categorical_accuracy: 0.4650\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.9874 - categorical_accuracy: 0.4848\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.8002 - categorical_accuracy: 0.7102\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7123 - categorical_accuracy: 0.7405\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.0106 - categorical_accuracy: 0.3977\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.8927 - categorical_accuracy: 0.5152\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7923 - categorical_accuracy: 0.5966\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.8762 - categorical_accuracy: 0.6278\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6677 - categorical_accuracy: 0.8068\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.8216 - categorical_accuracy: 0.5502\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6437 - categorical_accuracy: 0.8731\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5965 - categorical_accuracy: 0.8220\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5917 - categorical_accuracy: 0.7955\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5441 - categorical_accuracy: 0.7955\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4265 - categorical_accuracy: 0.8267\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.4706 - categorical_accuracy: 0.7756\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.4057 - categorical_accuracy: 0.8314\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6105 - categorical_accuracy: 0.7244\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.5473 - categorical_accuracy: 0.7197\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6882 - categorical_accuracy: 0.7045\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.3826 - categorical_accuracy: 0.2955\n",
      "Epoch 74/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 14ms/step - loss: 2.9362 - categorical_accuracy: 0.1837\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7432 - categorical_accuracy: 0.7462\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.2620 - categorical_accuracy: 0.1828\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7486 - categorical_accuracy: 0.7348\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.2233 - categorical_accuracy: 0.2244\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.2419 - categorical_accuracy: 0.2396\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.0064 - categorical_accuracy: 0.6477\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.0538 - categorical_accuracy: 0.6383\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.1406 - categorical_accuracy: 0.5568\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.1224 - categorical_accuracy: 0.5152\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 1.0163 - categorical_accuracy: 0.6894\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.9865 - categorical_accuracy: 0.6430\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.9800 - categorical_accuracy: 0.5975\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.9304 - categorical_accuracy: 0.5871\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.8824 - categorical_accuracy: 0.7547\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8364 - categorical_accuracy: 0.7045\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.8081 - categorical_accuracy: 0.7093\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.7283 - categorical_accuracy: 0.8466\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6541 - categorical_accuracy: 0.8674\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6345 - categorical_accuracy: 0.8011\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5404 - categorical_accuracy: 0.8674\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5451 - categorical_accuracy: 0.8314\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4644 - categorical_accuracy: 0.8419\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4633 - categorical_accuracy: 0.8419\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3658 - categorical_accuracy: 0.9233\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3967 - categorical_accuracy: 0.9233\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3238 - categorical_accuracy: 0.9186\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2879 - categorical_accuracy: 0.9337\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2461 - categorical_accuracy: 0.9441\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3808 - categorical_accuracy: 0.8419\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.3426 - categorical_accuracy: 0.8371\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5672 - categorical_accuracy: 0.7244\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2990 - categorical_accuracy: 0.8930\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1807 - categorical_accuracy: 0.9848\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.8967 - categorical_accuracy: 0.6420\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.6991 - categorical_accuracy: 0.7500\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.6892 - categorical_accuracy: 0.3665\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.7469 - categorical_accuracy: 0.6591\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6919 - categorical_accuracy: 0.6742\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7058 - categorical_accuracy: 0.6894\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.9816 - categorical_accuracy: 0.4839\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.8102 - categorical_accuracy: 0.6174\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6021 - categorical_accuracy: 0.7244\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.7260 - categorical_accuracy: 0.5966\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5378 - categorical_accuracy: 0.7652\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5959 - categorical_accuracy: 0.7197\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.4585 - categorical_accuracy: 0.8977\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5065 - categorical_accuracy: 0.8570\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4939 - categorical_accuracy: 0.8570\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4354 - categorical_accuracy: 0.9081\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4150 - categorical_accuracy: 0.9233\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3844 - categorical_accuracy: 0.9593\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3522 - categorical_accuracy: 0.9489\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3164 - categorical_accuracy: 0.9744\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3040 - categorical_accuracy: 0.9593\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2829 - categorical_accuracy: 0.9744\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2916 - categorical_accuracy: 0.9337\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2320 - categorical_accuracy: 0.9744\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2070 - categorical_accuracy: 0.9744\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1866 - categorical_accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1635 - categorical_accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1566 - categorical_accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1312 - categorical_accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1232 - categorical_accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1307 - categorical_accuracy: 0.9848\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0973 - categorical_accuracy: 0.9744\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1125 - categorical_accuracy: 0.9697\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1426 - categorical_accuracy: 0.9441\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1572 - categorical_accuracy: 0.9081\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1763 - categorical_accuracy: 0.9290\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2464 - categorical_accuracy: 0.9337\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.6294 - categorical_accuracy: 0.7812\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2192 - categorical_accuracy: 0.9394\n",
      "Epoch 147/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 17ms/step - loss: 0.6996 - categorical_accuracy: 0.6686\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3106 - categorical_accuracy: 0.8627\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.9731 - categorical_accuracy: 0.6629\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6654 - categorical_accuracy: 0.7140\n",
      "0\n",
      "-10\n",
      "-10\n",
      "-10\n",
      "-10\n",
      "-10\n",
      "-10\n",
      "-10\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-30\n",
      "-30\n",
      "-30\n",
      "-30\n",
      "-30\n",
      "-30\n",
      "-40\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import pygame\n",
    "mp_holistic = mp.solutions.holistic #holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils #drawing utilites\n",
    "def mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #BGR 2 RGB\n",
    "    image.flags.writeable = False                 #image is no longer writeable\n",
    "    results = model.process(image)                 #make prediction\n",
    "    image.flags.writeable = True                  #image is writeable\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR) #RGB 2 BGR\n",
    "    return image,results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image,results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) #draw pose connections\n",
    "    \n",
    "def extract_keypoints(results):\n",
    "    pose =[]\n",
    "    count=0\n",
    "    for res in results.pose_landmarks.landmark:\n",
    "        if(count==0):\n",
    "            test = np.array([res.x, res.y,res.z])\n",
    "            pose.append(test)\n",
    "        count+=1\n",
    "    pose = np.array(pose).flatten()\n",
    "    return pose    \n",
    "\n",
    "def extract_badkeypoints(results):\n",
    "    pose =[]\n",
    "    pose2 =[]\n",
    "    pose3 =[]\n",
    "    pose4 =[]\n",
    "    pose5 =[]\n",
    "    pose6 =[]\n",
    "    pose7 =[]\n",
    "    pose8 =[]\n",
    "    #대각선 4사분면\n",
    "    pose=(results[0]+0.18,results[1]+0.15,results[2])\n",
    "    #대각선 2사분면\n",
    "    pose2=(results[0]-0.18,results[1]-0.15,results[2])\n",
    "    #대각선 1사분면\n",
    "    pose3=(results[0]+0.18,results[1]-0.15,results[2])\n",
    "    #대각성 3사분면\n",
    "    pose4=(results[0]-0.18,results[1]+0.15,results[2])\n",
    "    #x축\n",
    "    pose5=(results[0]+0.18,results[1],results[2])\n",
    "    pose6=(results[0]-0.18,results[1],results[2])\n",
    "    #y축\n",
    "    pose7=(results[0],results[1]+0.1,results[2])\n",
    "    pose8=(results[0],results[1]-0.1,results[2])\n",
    "    \n",
    "    pose = np.array(pose).flatten()\n",
    "    pose2 = np.array(pose2).flatten()\n",
    "    pose3 = np.array(pose3).flatten()\n",
    "    pose4 = np.array(pose4).flatten()\n",
    "    pose5 = np.array(pose5).flatten()\n",
    "    pose6 = np.array(pose6).flatten()\n",
    "    pose7 = np.array(pose7).flatten()\n",
    "    pose8 = np.array(pose8).flatten()\n",
    "    return pose, pose2, pose3, pose4,pose5,pose6,pose7,pose8\n",
    "\n",
    "colors = [(245,117,16),(117,245,16)]\n",
    "\n",
    "modes = np.array(['mode0','mode1','mode2','reset','end'])\n",
    "\n",
    "\n",
    "def prob_viz(res,actions, input_frame, colors,mode,counter):\n",
    "    output_frame = input_frame.copy()\n",
    "    if np.argmax(res)==0:\n",
    "        num=0\n",
    "        prob=res[np.argmax(res)]\n",
    "    else:\n",
    "        num=1\n",
    "        prob=res[np.argmax(res)]   \n",
    "    cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100),90+num*40),colors[num],-1)\n",
    "    cv2.putText(output_frame,actions[num],(0,85+num*40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "    cv2.putText(output_frame,mode,(0,85+2*40),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,0),2,cv2.LINE_AA)\n",
    "    cv2.putText(output_frame,str(counter),(0,85+3*40),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,0),2,cv2.LINE_AA)\n",
    "    return output_frame\n",
    "\n",
    "#touch event\n",
    "def showBlank(event, x, y, flags, param):\n",
    "    #param is the array i from below\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        param[0] = param[0] + 1\n",
    "        cv2.imshow(\"OpenCV Feed\",image)\n",
    "    elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "        param[0] = param[0] + 1\n",
    "        cv2.imshow(\"OpenCV Feed\",image)\n",
    "    if x>0 and x<100 and y<25:\n",
    "        param[0]=-10\n",
    "    elif x>120 and x<200 and y<25:\n",
    "        param[0]=-20\n",
    "    elif x>240 and x<290 and y<25:\n",
    "        param[0]=-30\n",
    "    elif x>340 and x <400 and y<25:\n",
    "        param[0]=-40\n",
    "        \n",
    "# path for exproted data\n",
    "DATA_PATH = os.path.join(\"MP_DATA\")\n",
    "\n",
    "#Action that we try to detect\n",
    "actions = np.array(['good','bad','bad2','bad3','bad4','bad5','bad6','bad7','bad8'])\n",
    "\n",
    "#thirty videos worth of data\n",
    "no_sequences = 5\n",
    "\n",
    "#videos are goint to be 30 frames in length\n",
    "sequence_length =10\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "i = [0]\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"OpenCV Feed\", cv2.WND_PROP_FULLSCREEN)\n",
    "cv2.setWindowProperty(\"OpenCV Feed\",cv2.WND_PROP_FULLSCREEN,cv2.WINDOW_FULLSCREEN)\n",
    "cv2.setMouseCallback(\"OpenCV Feed\", showBlank, i )\n",
    "pygame.mixer.init()\n",
    "\n",
    "\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7) as holistic:\n",
    "    #LOOP through sequneces aka videos\n",
    "    for sequence in range(no_sequences):\n",
    "        #Loop through video length aka sequnece length\n",
    "        for frame_num in range(sequence_length):\n",
    "                \n",
    "            #Read feed\n",
    "            ret,frame = cap.read()\n",
    "            \n",
    "            #make detection\n",
    "            image , results  = mediapipe_detection(frame,holistic)\n",
    "            draw_landmarks(image,results)\n",
    "            #프로그램을 시작할 때 메시지와 음성을 출력해준다.\n",
    "            if sequence == 0 and frame_num==0:\n",
    "                cv2.putText(image,\"Touch the Screen.\",(100,100),cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,0,255), 4, cv2.LINE_AA)\n",
    "                cv2.imshow(\"OpenCV Feed\",image)\n",
    "                pygame.mixer.music.load('start.mp3')\n",
    "                pygame.mixer.music.play()\n",
    "                # show the initial image for the first time.\n",
    "                while i[0] < 1:    \n",
    "                    cv2.waitKey(10)\n",
    "                    cv2.imshow(\"OpenCV Feed\",image)\n",
    "                    \n",
    "    \n",
    "            #사용자의 모습이 보이지 않았을 경우 r키를 입력하여 다시 실행할 수 있게 한다.\n",
    "            try:\n",
    "                keypoints = extract_keypoints(results)\n",
    "                i[0]=5\n",
    "            except Exception as e:\n",
    "                i[0]=0\n",
    "                cv2.putText(image,\"Touch the Screen.\",(100,100),cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,0,255), 4, cv2.LINE_AA)\n",
    "                cv2.imshow(\"OpenCV Feed\",image)\n",
    "                cv2.setMouseCallback('OpenCV Feed', showBlank, i)\n",
    "\n",
    "            if i[0]<1:\n",
    "                while True:\n",
    "                    cv2.waitKey(10)\n",
    "                    cv2.imshow(\"OpenCV Feed\",image)\n",
    "                    if i[0] >1 or i[0]==1:\n",
    "                        break\n",
    "                frame_num=frame_num-1\n",
    "                continue\n",
    "            \n",
    "            #Apply collection logic\n",
    "            if frame_num == 0:\n",
    "                cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 4, cv2.LINE_AA)\n",
    "                    \n",
    "                cv2.putText(image, 'Collectiong frames for {} Video Number {}'.format(action,sequence), (15,12),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 4, cv2.LINE_AA)\n",
    "                #show to screen\n",
    "                cv2.imshow(\"OpenCV Feed\",image)\n",
    "                cv2.waitKey(1000)\n",
    "            else:\n",
    "                cv2.putText(image, 'Collectiong frames for {} Video Number {}'.format(action,sequence), (15,12),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 4,cv2.LINE_AA)\n",
    "                    \n",
    "            #show to screen\n",
    "            cv2.imshow(\"OpenCV Feed\",image)\n",
    "                  \n",
    "            #new export keypoints\n",
    "            keypoints = extract_keypoints(results)\n",
    "            npy_path=os.path.join(DATA_PATH, 'good', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,keypoints)\n",
    "            badkeypoints1,badkeypoints2, badkeypoints3, badkeypoints4, badkeypoints5, badkeypoints6, badkeypoints7, badkeypoints8 = extract_badkeypoints(keypoints)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints1)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad2', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints2)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad3', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints3)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad4', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints4)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad5', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints5)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad6', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints6)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad7', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints7)\n",
    "            npy_path=os.path.join(DATA_PATH, 'bad8', str(sequence),str(frame_num))\n",
    "            np.save(npy_path,badkeypoints8)\n",
    "\n",
    "#build train LSTM\n",
    "#crossentropy -> 수치로 표시하기에 유리한 방식으로 출력해주기 때문이다.\n",
    "#왜 이러한 구조로 구성하였나? -> \n",
    "#1. 적은 양의 데이터만 사용할 예정이고\n",
    "#2. 빠르게 학습시킬 수 있다는 장점과\n",
    "#3. 실시간으로 평가를 빠르게 내려줄 수 있기 때문입니다.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "sequences , labels = [],[]\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window=[]\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH,action,str(sequence),\"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "X = np.array(sequences)\n",
    "y= to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=1)\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64,return_sequences=True,activation='relu',input_shape=(10,3)))\n",
    "model.add(LSTM(128,return_sequences=True,activation='relu'))\n",
    "model.add(LSTM(64,return_sequences=False,activation='relu'))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "model.fit(X_train,y_train,epochs=150,callbacks=[tb_callback])\n",
    "model.save('action.h5')\n",
    "\n",
    "\n",
    "#실시간 탐지\n",
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions=[]\n",
    "threshold = 0.4\n",
    "bad_pose_count=0\n",
    "start =0\n",
    "mp3file='good.mp3'\n",
    "settime=10\n",
    "\n",
    "i[0]=0\n",
    "mode='mode0'\n",
    "\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7) as holistic:\n",
    "    while cap.isOpened():\n",
    "        #Read feed\n",
    "        ret,frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        #make detection\n",
    "        image , results  = mediapipe_detection(frame,holistic)\n",
    "        #draw_landmark\n",
    "        \n",
    "        #mode1(스트레칭 모드), mode2, reset(초기화), end(끝내기)\n",
    "        if i[0]!=5:\n",
    "            print(i[0])\n",
    "            if i[0]==-10:\n",
    "                mode='mode1'\n",
    "                settime=100\n",
    "            elif i[0]==-20:\n",
    "                mode='mode2'\n",
    "                settime=60\n",
    "            elif i[0]==-30:\n",
    "                mode='reset'\n",
    "                settime=10\n",
    "                bad_pose_count=0\n",
    "            elif i[0]==-40:\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                mode='end'\n",
    "            \n",
    "        #사용자의 모습이 보이지 않았을 경우 r키를 입력하여 다시 실행할 수 있게 한다.\n",
    "        try:\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.insert(0,keypoints)\n",
    "            sequence = sequence[:10]\n",
    "            i[0]=5\n",
    "            \n",
    "        except Exception as e:\n",
    "            i[0]=0\n",
    "            cv2.putText(image,\"Touch the Screen.\",(100,100),cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,0,255), 4, cv2.LINE_AA)\n",
    "            cv2.imshow(\"OpenCV Feed\",image)\n",
    "        \n",
    "        if i[0]==0:\n",
    "            while True:\n",
    "                cv2.waitKey(10)\n",
    "                cv2.putText(image,\"Touch the Screen.\",(100,100),cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,0,255), 4, cv2.LINE_AA)\n",
    "                cv2.imshow(\"OpenCV Feed\",image)\n",
    "                bad_pose_count=0\n",
    "                if i[0] >1 or i[0]==1:\n",
    "                    break\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            cv2.putText(image,\"Mode1\",(1,25),cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,0), 4, cv2.LINE_AA)\n",
    "            cv2.putText(image,\"Mode2\",(120,25),cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,0), 4, cv2.LINE_AA)\n",
    "            cv2.putText(image,\"Reset\",(240,25),cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,0), 4, cv2.LINE_AA)\n",
    "            cv2.putText(image,\"End\",(340,25),cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,0), 4, cv2.LINE_AA)        \n",
    "        \n",
    "        if len(sequence) == 10:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            #3. vizs logic - 0.4보다 큰 수치를 가졌을 경우에 상태가 바뀌면 바뀐 상태로, 안바뀌면 안바뀐 상태로 \n",
    "            if res[np.argmax(res)] > threshold:\n",
    "                if len(sentence) > 0:\n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                    #나쁜 자세가 1분 정도 지속적으로 유지되었을 때 음성으로 알려준다.\n",
    "                    if actions[np.argmax(res)] !=\"good\" and start == 0:\n",
    "                        start = time.time()\n",
    "                    elif actions[np.argmax(res)] == \"good\":\n",
    "                        start = 0\n",
    "                    elif actions[np.argmax(res)] !=\"good\" :\n",
    "                        dif=time.time()-start\n",
    "                        if dif > settime:\n",
    "                            pygame.mixer.music.load(mp3file)\n",
    "                            pygame.mixer.music.play()\n",
    "                            start=time.time()\n",
    "                            dif = 0\n",
    "                            bad_pose_count=bad_pose_count+1\n",
    "                            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                            cv2.imshow(\"OpenCV Feed\",image)\n",
    "                            cv2.waitKey(100)\n",
    "\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence)>5:\n",
    "                sentence = sentence[-5:]\n",
    "            \n",
    "            #viz\n",
    "            image = prob_viz(res,actions,image,colors,mode,bad_pose_count)\n",
    "            #show to screen\n",
    "            cv2.imshow(\"OpenCV Feed\",image)\n",
    "            \n",
    "        #breaking\n",
    "        if (cv2.waitKey(10) & 0xFF == ord('q')) or mode=='end':\n",
    "            break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a04f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566cd827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.7.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "371924ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions=[]\n",
    "threshold = 0.4\n",
    "bad_pose_count=0\n",
    "start =0\n",
    "\n",
    "        \n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"OpenCV Feed\", cv2.WND_PROP_FULLSCREEN)\n",
    "cv2.setWindowProperty(\"OpenCV Feed\",cv2.WND_PROP_FULLSCREEN,cv2.WINDOW_FULLSCREEN)\n",
    "cv2.setMouseCallback('OpenCV Feed', showBlank, i)\n",
    "i=[0]\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7) as holistic:\n",
    "    while cap.isOpened():\n",
    "        #Read feed\n",
    "        ret,frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            continue\n",
    "            \n",
    "        #make detection\n",
    "        image , results  = mediapipe_detection(frame,holistic)\n",
    "        #draw_landmark\n",
    "        #사용자의 모습이 보이지 않았을 경우 r키를 입력하여 다시 실행할 수 있게 한다.\n",
    "        try:\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.insert(0,keypoints)\n",
    "            sequence = sequence[:10]\n",
    "            i[0]=5\n",
    "            \n",
    "        except Exception as e:\n",
    "            i[0]=0\n",
    "            cv2.putText(image,\"Touch the Screen.\",(100,100),cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,0,255), 4, cv2.LINE_AA)\n",
    "            cv2.imshow(\"OpenCV Feed\",image)\n",
    "        \n",
    "        if i[0]<1:\n",
    "            while True:\n",
    "                cv2.waitKey(10)\n",
    "                cv2.imshow(\"OpenCV Feed\",image)\n",
    "                if i[0] >1 or i[0]==1:\n",
    "                    break\n",
    "            continue\n",
    "        else:\n",
    "            cv2.putText(image,\"Menu\",(1,25),cv2.FONT_HERSHEY_SIMPLEX,1, (255,255,255), 4, cv2.LINE_AA)\n",
    "                \n",
    "        if len(sequence) == 10:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "        \n",
    "            #3. vizs logic - 0.4보다 큰 수치를 가졌을 경우에 상태가 바뀌면 바뀐 상태로, 안바뀌면 안바뀐 상태로 \n",
    "            if res[np.argmax(res)] > threshold:\n",
    "                if len(sentence) > 0:\n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                    #나쁜 자세가 5분 정도 지속적으로 유지되었을 때 음성으로 알려준다.\n",
    "                    if actions[np.argmax(res)] !=\"good\" and start == 0:\n",
    "                        start = time.time()\n",
    "                    elif actions[np.argmax(res)] == \"good\":\n",
    "                        start = 0\n",
    "                    elif actions[np.argmax(res)] !=\"good\" :\n",
    "                        dif=time.time()-start\n",
    "                        if dif > 60:\n",
    "                            pygame.mixer.music.load('good.mp3')\n",
    "                            pygame.mixer.music.play()\n",
    "                            start=time.time()\n",
    "                            dif = 0\n",
    "\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence)>5:\n",
    "                sentence = sentence[-5:]\n",
    "            \n",
    "            #viz\n",
    "            image = prob_viz(res,actions,image,colors)\n",
    "            #show to screen\n",
    "        cv2.imshow(\"OpenCV Feed\",image)\n",
    "            \n",
    "        #breaking\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79d1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
